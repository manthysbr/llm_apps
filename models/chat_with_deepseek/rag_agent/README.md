# HeraCorps RAG (Retrieval-Augmented Generation) Agent

## ğŸš€ VisÃ£o Geral  
Este protÃ³tipo implementa um agente de **RAG (Retrievalâ€‘Augmented Generation)** em Python, combinando:
- Streamlit como UI interativa de chat  
- FAISS para armazenamento vetorial e busca semÃ¢ntica  
- HuggingFace Embeddings (`allâ€‘MiniLMâ€‘L6â€‘v2`)  
- LLM local (Ollama/deepseekâ€‘r1:14b) orquestrado via LangChain  

O objetivo Ã© permitir que pesquisadores carreguem documentos (PDFs), indexem o conteÃºdo e faÃ§am consultas em linguagem natural, recebendo respostas fundamentadas no corpus carregado.

## ğŸ¯ Problema EndereÃ§ado  
- Dificuldade de extrair informaÃ§Ã£o de grandes volumes de documentos legais, tÃ©cnicos ou de pesquisa.  
- Ferramentas tradicionais de busca nÃ£o capturam semÃ¢ntica e contexto de forma robusta.  
- Necessidade de protÃ³tipos de pesquisa para avaliar integraÃ§Ã£o LLM + vector store em pipelines RAG.

## ğŸ— Arquitetura & Fluxo de Dados  
1. **Upload de Documentos**  
   - UsuÃ¡rio faz upload de mÃºltiplos PDFs via Streamlit.  
   - Cada arquivo Ã© salvo temporariamente e carregado com `PyPDFLoader`.  
2. **PrÃ©â€‘processamento**  
   - Split de texto em chunks de 1000 tokens, overlap de 200 (via `RecursiveCharacterTextSplitter`).  
3. **IndexaÃ§Ã£o Vetorial**  
   - VetorizaÃ§Ã£o com `HuggingFaceEmbeddings` sobre cada chunk.  
   - CriaÃ§Ã£o de Ã­ndice FAISS onâ€‘memory (`FAISS.from_documents`).  
4. **Chat Interativo**  
   - HistÃ³rico mantido em `st.session_state.chat_history`.  
   - No envio de uma pergunta:
     - Busca semÃ¢ntica (`similarity_search(k=3)`) retorna contextos relevantes.  
     - ConstruÃ§Ã£o de prompt LangChain (`ChatPromptTemplate`) com bloco SYSTEM + HUMAN.  
     - GeraÃ§Ã£o de resposta via Ollama e parsing por `StrOutputParser`.  
     - ExtraÃ§Ã£o de raciocÃ­nio interno (`<think>â€¦</think>`) por regex.  
     - ExibiÃ§Ã£o da resposta, raciocÃ­nio e fontes usadas.

```
[User Query] â”€â–º FAISS Retrieve â”€â–º Context â–º Prompt â”‚
                                â†“                 â–¼
                             Ollama LLM â”€â–º Parsed Response â”€â–º Streamlit UI
```

## ğŸ” Componentes Principais  
- **rag.py**: aplicaÃ§Ã£o Streamlit Ãºnica para toda a pipeline.  
- **Streamlit**: interface de upload, chat e exibiÃ§Ã£o de histÃ³rico.  
- **LangChain**  
  - `ChatPromptTemplate.from_messages()`  
  - `StrOutputParser()`  
  - `RecursiveCharacterTextSplitter`  
  - `PyPDFLoader`  
- **FAISS**: similaridade vetorial semÃ¢ntica.  
- **HuggingFaceEmbeddings**: geraÃ§Ã£o de vetores de alta qualidade.  
- **Ollama (deepseekâ€‘r1:14b)**: modelo onâ€‘premise de geraÃ§Ã£o de linguagem.

## ğŸ“¦ Tecnologias & Bibliotecas  
- Python 3.8+  
- Streamlit  
- langchainâ€‘community & langchainâ€‘core  
- Ollama  
- FAISS  
- HuggingFace Embeddings  
- PyPDFLoader  
- Regex & builtâ€‘in Python I/O  

## âš¡ï¸ Como Executar  
```bash
git clone https://github.com/manthysbr/llm_apps.git
cd llm_apps/models/chat_with_deepseek/rag_agent
pip install -r requirements.txt
streamlit run rag.py
```

ApÃ³s inicializar, carregue arquivos PDF, aguarde o processamento e interaja via chat para obter respostas baseadas no conteÃºdo indexado.

## ğŸ”­ ExtensÃµes Futuras  
- PersistÃªncia do Ã­ndice FAISS em disco ou Qdrant  
- Suporte a outros formatos (Word, TXT, HTML)  
- IntegraÃ§Ã£o com autenticaÃ§Ã£o e ACL  
- Monitoramento de latÃªncia e mÃ©tricas de relevÃ¢ncia  
- AdoÃ§Ã£o de functionâ€‘calling para executar aÃ§Ãµes a partir de respostas  

---
Feito com â¤ï¸ pela HeraCorps AI Division  
